{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.3966,  0.0474,  0.2057],\n",
      "        [-0.2605, -0.1229, -0.2295]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.2230, -0.4970], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[-1.6080, -0.0357,  2.3119],\n",
      "        [-0.1782,  1.2184, -2.1049]])\n",
      "dL/db:  tensor([0.7915, 0.0525])\n",
      "dL/dw:  tensor([[-2.3513, -0.0663,  3.3898],\n",
      "        [-0.2765,  1.8054, -3.1115]])\n",
      "dL/db:  tensor([1.1547, 0.0976])\n",
      "dL/dw:  tensor([[-3.0239, -0.1117,  4.3763],\n",
      "        [-0.3853,  2.3658, -4.0643]])\n",
      "dL/db:  tensor([1.4797, 0.1645])\n",
      "dL/dw:  tensor([[-3.6055, -0.1756,  5.2451],\n",
      "        [-0.5072,  2.8916, -4.9472]])\n",
      "dL/db:  tensor([1.7555, 0.2596])\n",
      "dL/dw:  tensor([[-4.0789, -0.2611,  5.9732],\n",
      "        [-0.6444,  3.3750, -5.7457]])\n",
      "dL/db:  tensor([1.9725, 0.3884])\n",
      "dL/dw:  tensor([[-4.4299, -0.3701,  6.5414],\n",
      "        [-0.7978,  3.8086, -6.4472]])\n",
      "dL/db:  tensor([2.1226, 0.5552])\n",
      "dL/dw:  tensor([[-4.6482, -0.5035,  6.9352],\n",
      "        [-0.9680,  4.1861, -7.0412]])\n",
      "dL/db:  tensor([2.1996, 0.7634])\n",
      "dL/dw:  tensor([[-4.7276, -0.6610,  7.1447],\n",
      "        [-1.1542,  4.5016, -7.5197]])\n",
      "dL/db:  tensor([2.1995, 1.0150])\n",
      "dL/dw:  tensor([[-4.6662, -0.8410,  7.1656],\n",
      "        [-1.3547,  4.7503, -7.8769]])\n",
      "dL/db:  tensor([2.1201, 1.3107])\n",
      "dL/dw:  tensor([[-4.4662, -1.0406,  6.9984],\n",
      "        [-1.5669,  4.9284, -8.1099]])\n",
      "dL/db:  tensor([1.9618, 1.6497])\n",
      "dL/dw:  tensor([[-4.1344, -1.2559,  6.6493],\n",
      "        [-1.7874,  5.0329, -8.2182]])\n",
      "dL/db:  tensor([1.7270, 2.0299])\n",
      "dL/dw:  tensor([[-3.6815, -1.4819,  6.1291],\n",
      "        [-2.0116,  5.0620, -8.2041]])\n",
      "dL/db:  tensor([1.4203, 2.4478])\n",
      "dL/dw:  tensor([[-3.1219, -1.7125,  5.4535],\n",
      "        [-2.2344,  5.0152, -8.0723]])\n",
      "dL/db:  tensor([1.0481, 2.8985])\n",
      "dL/dw:  tensor([[-2.4735, -1.9411,  4.6427],\n",
      "        [-2.4502,  4.8930, -7.8296]])\n",
      "dL/db:  tensor([0.6191, 3.3759])\n",
      "dL/dw:  tensor([[-1.7568, -2.1604,  3.7201],\n",
      "        [-2.6527,  4.6972, -7.4853]])\n",
      "dL/db:  tensor([0.1431, 3.8728])\n",
      "dL/dw:  tensor([[-0.9946, -2.3631,  2.7124],\n",
      "        [-2.8354,  4.4306, -7.0502]])\n",
      "dL/db:  tensor([-0.3683,  4.3808])\n",
      "dL/dw:  tensor([[-0.2112, -2.5415,  1.6484],\n",
      "        [-2.9921,  4.0973, -6.5368]])\n",
      "dL/db:  tensor([-0.9028,  4.8910])\n",
      "dL/dw:  tensor([[ 0.5686, -2.6884,  0.5583],\n",
      "        [-3.1164,  3.7027, -5.9588]])\n",
      "dL/db:  tensor([-1.4473,  5.3936])\n",
      "dL/dw:  tensor([[ 1.3197, -2.7970, -0.5274],\n",
      "        [-3.2023,  3.2530, -5.3305]])\n",
      "dL/db:  tensor([-1.9883,  5.8788])\n",
      "dL/dw:  tensor([[ 2.0180, -2.8614, -1.5784],\n",
      "        [-3.2448,  2.7556, -4.6671]])\n",
      "dL/db:  tensor([-2.5123,  6.3361])\n",
      "dL/dw:  tensor([[ 2.6408, -2.8766, -2.5657],\n",
      "        [-3.2391,  2.2187, -3.9834]])\n",
      "dL/db:  tensor([-3.0063,  6.7554])\n",
      "dL/dw:  tensor([[ 3.1678, -2.8388, -3.4624],\n",
      "        [-3.1820,  1.6514, -3.2941]])\n",
      "dL/db:  tensor([-3.4579,  7.1269])\n",
      "dL/dw:  tensor([[ 3.5813, -2.7456, -4.2443],\n",
      "        [-3.0709,  1.0633, -2.6134]])\n",
      "dL/db:  tensor([-3.8559,  7.4411])\n",
      "dL/dw:  tensor([[ 3.8673, -2.5963, -4.8910],\n",
      "        [-2.9047,  0.4647, -1.9542]])\n",
      "dL/db:  tensor([-4.1901,  7.6893])\n",
      "dL/dw:  tensor([[ 4.0153, -2.3918, -5.3860],\n",
      "        [-2.6837, -0.1338, -1.3284]])\n",
      "dL/db:  tensor([-4.4523,  7.8639])\n",
      "dL/dw:  tensor([[ 4.0193, -2.1345, -5.7174],\n",
      "        [-2.4094, -0.7216, -0.7461]])\n",
      "dL/db:  tensor([-4.6358,  7.9580])\n",
      "dL/dw:  tensor([[ 3.8774, -1.8286, -5.8780],\n",
      "        [-2.0848, -1.2879, -0.2158]])\n",
      "dL/db:  tensor([-4.7361,  7.9663])\n",
      "dL/dw:  tensor([[ 3.5924, -1.4801, -5.8659],\n",
      "        [-1.7143, -1.8221,  0.2557]])\n",
      "dL/db:  tensor([-4.7506,  7.8848])\n",
      "dL/dw:  tensor([[ 3.1714, -1.0962, -5.6839],\n",
      "        [-1.3036, -2.3143,  0.6641]])\n",
      "dL/db:  tensor([-4.6788,  7.7110])\n",
      "dL/dw:  tensor([[ 2.6256, -0.6857, -5.3401],\n",
      "        [-0.8594, -2.7549,  1.0066]])\n",
      "dL/db:  tensor([-4.5225,  7.4438])\n",
      "dL/dw:  tensor([[ 1.9703, -0.2583, -4.8470],\n",
      "        [-0.3897, -3.1353,  1.2828]])\n",
      "dL/db:  tensor([-4.2855,  7.0839])\n",
      "dL/dw:  tensor([[ 1.2242,  0.1752, -4.2214],\n",
      "        [ 0.0968, -3.4478,  1.4941]])\n",
      "dL/db:  tensor([-3.9733,  6.6335])\n",
      "dL/dw:  tensor([[ 0.4087,  0.6036, -3.4838],\n",
      "        [ 0.5908, -3.6858,  1.6437]])\n",
      "dL/db:  tensor([-3.5934,  6.0963])\n",
      "dL/dw:  tensor([[-0.4523,  1.0153, -2.6580],\n",
      "        [ 1.0826, -3.8441,  1.7365]])\n",
      "dL/db:  tensor([-3.1548,  5.4776])\n",
      "dL/dw:  tensor([[-1.3334,  1.3988, -1.7699],\n",
      "        [ 1.5621, -3.9189,  1.7789]])\n",
      "dL/db:  tensor([-2.6677,  4.7841])\n",
      "dL/dw:  tensor([[-2.2082,  1.7431, -0.8472],\n",
      "        [ 2.0197, -3.9081,  1.7783]])\n",
      "dL/db:  tensor([-2.1433,  4.0238])\n",
      "dL/dw:  tensor([[-3.0506,  2.0378,  0.0817],\n",
      "        [ 2.4460, -3.8110,  1.7433]])\n",
      "dL/db:  tensor([-1.5936,  3.2058])\n",
      "dL/dw:  tensor([[-3.8349,  2.2736,  0.9885],\n",
      "        [ 2.8321, -3.6288,  1.6830]])\n",
      "dL/db:  tensor([-1.0309,  2.3402])\n",
      "dL/dw:  tensor([[-4.5372,  2.4425,  1.8458],\n",
      "        [ 3.1702, -3.3643,  1.6066]])\n",
      "dL/db:  tensor([-0.4675,  1.4378])\n",
      "dL/dw:  tensor([[-5.1355,  2.5383,  2.6279],\n",
      "        [ 3.4535, -3.0221,  1.5236]])\n",
      "dL/db:  tensor([0.0847, 0.5102])\n",
      "dL/dw:  tensor([[-5.6109,  2.5566,  3.3114],\n",
      "        [ 3.6764, -2.6084,  1.4431]])\n",
      "dL/db:  tensor([ 0.6140, -0.4308])\n",
      "dL/dw:  tensor([[-5.9481,  2.4949,  3.8765],\n",
      "        [ 3.8349, -2.1308,  1.3737]])\n",
      "dL/db:  tensor([ 1.1101, -1.3730])\n",
      "dL/dw:  tensor([[-6.1353,  2.3532,  4.3066],\n",
      "        [ 3.9266, -1.5985,  1.3230]])\n",
      "dL/db:  tensor([ 1.5634, -2.3045])\n",
      "dL/dw:  tensor([[-6.1655,  2.1334,  4.5896],\n",
      "        [ 3.9505, -1.0220,  1.2977]])\n",
      "dL/db:  tensor([ 1.9661, -3.2135])\n",
      "dL/dw:  tensor([[-6.0358,  1.8398,  4.7178],\n",
      "        [ 3.9075, -0.4130,  1.3031]])\n",
      "dL/db:  tensor([ 2.3115, -4.0885])\n",
      "dL/dw:  tensor([[-5.7482,  1.4789,  4.6884],\n",
      "        [ 3.8002,  0.2163,  1.3433]])\n",
      "dL/db:  tensor([ 2.5949, -4.9187])\n",
      "dL/dw:  tensor([[-5.3092,  1.0592,  4.5032],\n",
      "        [ 3.6327,  0.8527,  1.4207]])\n",
      "dL/db:  tensor([ 2.8132, -5.6943])\n",
      "dL/dw:  tensor([[-4.7298,  0.5911,  4.1687],\n",
      "        [ 3.4106,  1.4828,  1.5363]])\n",
      "dL/db:  tensor([ 2.9652, -6.4060])\n",
      "dL/dw:  tensor([[-4.0251,  0.0867,  3.6959],\n",
      "        [ 3.1409,  2.0929,  1.6895]])\n",
      "dL/db:  tensor([ 3.0514, -7.0458])\n",
      "dL/dw:  tensor([[-3.2138, -0.4407,  3.1000],\n",
      "        [ 2.8318,  2.6696,  1.8780]])\n",
      "dL/db:  tensor([ 3.0739, -7.6069])\n",
      "dL/dw:  tensor([[-2.3178, -0.9768,  2.3999],\n",
      "        [ 2.4923,  3.2000,  2.0982]])\n",
      "dL/db:  tensor([ 3.0366, -8.0835])\n",
      "dL/dw:  tensor([[-1.3617, -1.5066,  1.6173],\n",
      "        [ 2.1321,  3.6719,  2.3453]])\n",
      "dL/db:  tensor([ 2.9446, -8.4715])\n",
      "dL/dw:  tensor([[-0.3715, -2.0149,  0.7765],\n",
      "        [ 1.7615,  4.0741,  2.6132]])\n",
      "dL/db:  tensor([ 2.8045, -8.7675])\n",
      "dL/dw:  tensor([[ 0.6252, -2.4867, -0.0965],\n",
      "        [ 1.3907,  4.3965,  2.8949]])\n",
      "dL/db:  tensor([ 2.6235, -8.9700])\n",
      "dL/dw:  tensor([[ 1.6009, -2.9077, -0.9750],\n",
      "        [ 1.0298,  4.6308,  3.1825]])\n",
      "dL/db:  tensor([ 2.4098, -9.0786])\n",
      "dL/dw:  tensor([[ 2.5287, -3.2646, -1.8322],\n",
      "        [ 0.6885,  4.7702,  3.4679]])\n",
      "dL/db:  tensor([ 2.1721, -9.0939])\n",
      "dL/dw:  tensor([[ 3.3829, -3.5454, -2.6419],\n",
      "        [ 0.3756,  4.8098,  3.7426]])\n",
      "dL/db:  tensor([ 1.9191, -9.0181])\n",
      "dL/dw:  tensor([[ 4.1398, -3.7399, -3.3796],\n",
      "        [ 0.0991,  4.7468,  3.9981]])\n",
      "dL/db:  tensor([ 1.6597, -8.8542])\n",
      "dL/dw:  tensor([[ 4.7787, -3.8400, -4.0230],\n",
      "        [-0.1343,  4.5805,  4.2263]])\n",
      "dL/db:  tensor([ 1.4022, -8.6065])\n",
      "dL/dw:  tensor([[ 5.2822, -3.8399, -4.5527],\n",
      "        [-0.3195,  4.3122,  4.4195]])\n",
      "dL/db:  tensor([ 1.1545, -8.2801])\n",
      "dL/dw:  tensor([[ 5.6369, -3.7365, -4.9528],\n",
      "        [-0.4528,  3.9456,  4.5706]])\n",
      "dL/db:  tensor([ 0.9236, -7.8808])\n",
      "dL/dw:  tensor([[ 5.8337, -3.5291, -5.2113],\n",
      "        [-0.5323,  3.4865,  4.6737]])\n",
      "dL/db:  tensor([ 0.7156, -7.4152])\n",
      "dL/dw:  tensor([[ 5.8680, -3.2198, -5.3206],\n",
      "        [-0.5576,  2.9425,  4.7237]])\n",
      "dL/db:  tensor([ 0.5352, -6.8906])\n",
      "dL/dw:  tensor([[ 5.7401, -2.8137, -5.2774],\n",
      "        [-0.5305,  2.3234,  4.7170]])\n",
      "dL/db:  tensor([ 0.3863, -6.3143])\n",
      "dL/dw:  tensor([[ 5.4550, -2.3180, -5.0833],\n",
      "        [-0.4543,  1.6405,  4.6510]])\n",
      "dL/db:  tensor([ 0.2710, -5.6944])\n",
      "dL/dw:  tensor([[ 5.0221, -1.7429, -4.7441],\n",
      "        [-0.3340,  0.9067,  4.5248]])\n",
      "dL/db:  tensor([ 0.1903, -5.0388])\n",
      "dL/dw:  tensor([[ 4.4556, -1.1006, -4.2700],\n",
      "        [-0.1759,  0.1362,  4.3384]])\n",
      "dL/db:  tensor([ 0.1439, -4.3558])\n",
      "dL/dw:  tensor([[ 3.7731, -0.4052, -3.6752],\n",
      "        [ 0.0120, -0.6560,  4.0937]])\n",
      "dL/db:  tensor([ 0.1301, -3.6533])\n",
      "dL/dw:  tensor([[ 2.9958,  0.3274, -2.9775],\n",
      "        [ 0.2209, -1.4540,  3.7934]])\n",
      "dL/db:  tensor([ 0.1460, -2.9393])\n",
      "dL/dw:  tensor([[ 2.1479,  1.0803, -2.1976],\n",
      "        [ 0.4410, -2.2419,  3.4415]])\n",
      "dL/db:  tensor([ 0.1878, -2.2214])\n",
      "dL/dw:  tensor([[ 1.2555,  1.8356, -1.3585],\n",
      "        [ 0.6619, -3.0035,  3.0432]])\n",
      "dL/db:  tensor([ 0.2506, -1.5071])\n",
      "dL/dw:  tensor([[ 0.3459,  2.5751, -0.4850],\n",
      "        [ 0.8732, -3.7232,  2.6044]])\n",
      "dL/db:  tensor([ 0.3292, -0.8033])\n",
      "dL/dw:  tensor([[-0.5529,  3.2809,  0.3974],\n",
      "        [ 1.0643, -4.3860,  2.1319]])\n",
      "dL/db:  tensor([ 0.4176, -0.1166])\n",
      "dL/dw:  tensor([[-1.4133,  3.9356,  1.2631],\n",
      "        [ 1.2250, -4.9780,  1.6326]])\n",
      "dL/db:  tensor([0.5096, 0.5471])\n",
      "dL/dw:  tensor([[-2.2087,  4.5228,  2.0869],\n",
      "        [ 1.3460, -5.4865,  1.1142]])\n",
      "dL/db:  tensor([0.5992, 1.1822])\n",
      "dL/dw:  tensor([[-2.9146,  5.0277,  2.8454],\n",
      "        [ 1.4187, -5.9006,  0.5841]])\n",
      "dL/db:  tensor([0.6805, 1.7839])\n",
      "dL/dw:  tensor([[-3.5090,  5.4373,  3.5172],\n",
      "        [ 1.4360, -6.2111,  0.0499]])\n",
      "dL/db:  tensor([0.7481, 2.3477])\n",
      "dL/dw:  tensor([[-3.9731,  5.7411,  4.0836],\n",
      "        [ 1.3920, -6.4109, -0.4812]])\n",
      "dL/db:  tensor([0.7969, 2.8700])\n",
      "dL/dw:  tensor([[-4.2922,  5.9307,  4.5293],\n",
      "        [ 1.2827, -6.4954, -1.0027]])\n",
      "dL/db:  tensor([0.8232, 3.3475])\n",
      "dL/dw:  tensor([[-4.4557,  6.0009,  4.8427],\n",
      "        [ 1.1058, -6.4619, -1.5083]])\n",
      "dL/db:  tensor([0.8237, 3.7778])\n",
      "dL/dw:  tensor([[-4.4577,  5.9492,  5.0166],\n",
      "        [ 0.8610, -6.3106, -1.9926]])\n",
      "dL/db:  tensor([0.7965, 4.1589])\n",
      "dL/dw:  tensor([[-4.2969,  5.7762,  5.0477],\n",
      "        [ 0.5501, -6.0439, -2.4512]])\n",
      "dL/db:  tensor([0.7407, 4.4894])\n",
      "dL/dw:  tensor([[-3.9772,  5.4853,  4.9375],\n",
      "        [ 0.1765, -5.6664, -2.8802]])\n",
      "dL/db:  tensor([0.6566, 4.7686])\n",
      "dL/dw:  tensor([[-3.5068,  5.0831,  4.6916],\n",
      "        [-0.2541, -5.1854, -3.2770]])\n",
      "dL/db:  tensor([0.5455, 4.9961])\n",
      "dL/dw:  tensor([[-2.8987,  4.5787,  4.3197],\n",
      "        [-0.7344, -4.6102, -3.6397]])\n",
      "dL/db:  tensor([0.4101, 5.1723])\n",
      "dL/dw:  tensor([[-2.1700,  3.9840,  3.8355],\n",
      "        [-1.2554, -3.9520, -3.9673]])\n",
      "dL/db:  tensor([0.2538, 5.2980])\n",
      "dL/dw:  tensor([[-1.3413,  3.3129,  3.2559],\n",
      "        [-1.8068, -3.2237, -4.2595]])\n",
      "dL/db:  tensor([0.0811, 5.3743])\n",
      "dL/dw:  tensor([[-0.4365,  2.5813,  2.6008],\n",
      "        [-2.3770, -2.4399, -4.5169]])\n",
      "dL/db:  tensor([-0.1027,  5.4031])\n",
      "dL/dw:  tensor([[ 0.5183,  1.8065,  1.8923],\n",
      "        [-2.9536, -1.6160, -4.7406]])\n",
      "dL/db:  tensor([-0.2920,  5.3864])\n",
      "dL/dw:  tensor([[ 1.4955,  1.0070,  1.1538],\n",
      "        [-3.5235, -0.7686, -4.9321]])\n",
      "dL/db:  tensor([-0.4807,  5.3268])\n",
      "dL/dw:  tensor([[ 2.4664,  0.2016,  0.4097],\n",
      "        [-4.0735,  0.0855, -5.0934]])\n",
      "dL/db:  tensor([-0.6626,  5.2272])\n",
      "dL/dw:  tensor([[ 3.4025, -0.5907, -0.3157],\n",
      "        [-4.5906,  0.9294, -5.2264]])\n",
      "dL/db:  tensor([-0.8314,  5.0907])\n",
      "dL/dw:  tensor([[ 4.2763, -1.3513, -0.9988],\n",
      "        [-5.0619,  1.7462, -5.3332]])\n",
      "dL/db:  tensor([-0.9814,  4.9209])\n",
      "dL/dw:  tensor([[ 5.0615, -2.0625, -1.6174],\n",
      "        [-5.4756,  2.5198, -5.4156]])\n",
      "dL/db:  tensor([-1.1071,  4.7215])\n",
      "dL/dw:  tensor([[ 5.7348, -2.7080, -2.1517],\n",
      "        [-5.8208,  3.2352, -5.4755]])\n",
      "dL/db:  tensor([-1.2040,  4.4963])\n",
      "dL/dw:  tensor([[ 6.2755, -3.2733, -2.5846],\n",
      "        [-6.0881,  3.8782, -5.5139]])\n",
      "dL/db:  tensor([-1.2683,  4.2496])\n",
      "dL/dw:  tensor([[ 6.6667, -3.7459, -2.9023],\n",
      "        [-6.2697,  4.4367, -5.5317]])\n",
      "dL/db:  tensor([-1.2972,  3.9853])\n",
      "dL/dw:  tensor([[ 6.8960, -4.1158, -3.0948],\n",
      "        [-6.3594,  4.9001, -5.5291]])\n",
      "dL/db:  tensor([-1.2891,  3.7077])\n",
      "dL/dw:  tensor([[ 6.9552, -4.3756, -3.1562],\n",
      "        [-6.3534,  5.2598, -5.5057]])\n",
      "dL/db:  tensor([-1.2437,  3.4209])\n",
      "dL/dw:  tensor([[ 6.8409, -4.5210, -3.0850],\n",
      "        [-6.2496,  5.5096, -5.4605]])\n",
      "dL/db:  tensor([-1.1618,  3.1290])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass.\n",
    "for i in range(100):\n",
    "    pred = linear(Variable(x))\n",
    "    \n",
    "    #compute loss.\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print out the gradients.\n",
    "    print ('dL/dw: ', linear.weight.grad) \n",
    "    print ('dL/db: ', linear.bias.grad)\n",
    "    \n",
    "    # 1-step gradient descent.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6111488342285156\n"
     ]
    }
   ],
   "source": [
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[-0.8163, -0.0152,  1.1718],\n",
      "        [-0.0872,  0.6137, -1.0617]])\n",
      "dL/db:  tensor([0.4024, 0.0225])\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-step gradient descent.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimization:  1.5744489431381226\n"
     ]
    }
   ],
   "source": [
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Loading data from numpy\n",
    "x_np = np.array([[1,2], [3,4]])\n",
    "x_t = torch.from_numpy(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 1), (15, 1))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters - 사용자가 임의로 주는 파라미터\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60       # 60번 진행\n",
    "learning_rate = 0.001 # gradient 비율을 0.001로 하려고?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.9837\n",
      "Epoch [10/60], Loss: 0.5014\n",
      "Epoch [15/60], Loss: 0.3060\n",
      "Epoch [20/60], Loss: 0.2268\n",
      "Epoch [25/60], Loss: 0.1947\n",
      "Epoch [30/60], Loss: 0.1817\n",
      "Epoch [35/60], Loss: 0.1764\n",
      "Epoch [40/60], Loss: 0.1743\n",
      "Epoch [45/60], Loss: 0.1734\n",
      "Epoch [50/60], Loss: 0.1731\n",
      "Epoch [55/60], Loss: 0.1729\n",
      "Epoch [60/60], Loss: 0.1728\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "\n",
    "model_path = '/home/cyc/deep_learning'\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/cyc/deep_learning/abe.'\n",
    "torch.save(model.state_dict(), model_path) # 모델에서 parameter만 저장하면 용량도 절약되고 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nnmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.cuda.is_available() # gpu가 사용가능한지 확인하는 메소드, True을 경우 사용가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100      # 100개를 뽑겠다.\n",
    "learning_rate = 0.001 # 나의 판단으로 0.001을 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "# linear/선형변환을 일일히 해주기 싫어서 클래스를 만들어서 한번에 선형변환을 하는 듯 하다.\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU() # activation function is relu\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.2593\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1414\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1621\n",
      "Epoch [1/5], Step [400/600], Loss: 0.2505\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1886\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1631\n",
      "Epoch [2/5], Step [100/600], Loss: 0.2700\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0847\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0988\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1434\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1084\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1094\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0391\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0320\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0641\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0606\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0707\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0578\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0737\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0894\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0686\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0239\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0717\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0851\n",
      "Epoch [5/5], Step [100/600], Loss: 0.1403\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0203\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0527\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0376\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0082\n",
      "Epoch [5/5], Step [600/600], Loss: 0.1268\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss is negative likelihood\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.04 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
